{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e813718",
   "metadata": {},
   "source": [
    "## preb Intern Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### read datasets ####################################\n",
    "import pandas as pd \n",
    "df_intern = pd.read_csv('data_0.0_raw.csv', sep=';')\n",
    "df_extern = pd.read_csv('dataWether.csv', sep=';')\n",
    "\n",
    "#################################### create new attributes ####################################\n",
    "#transform to unixtime\n",
    "df_intern['chargingstart'] = pd.to_datetime(df_intern['chargingstart'])\n",
    "df_intern['chargingstop'] = pd.to_datetime(df_intern['chargingstop'])\n",
    "#create new attribute chargingtime\n",
    "df_intern['chargingtime'] = df_intern['chargingstop'] - df_intern['chargingstart']\n",
    "df_intern['chargingtime'] = df_intern['chargingtime'].dt.total_seconds().div(60).astype(int)\n",
    "#create new feature hour, dayOfWeek, month, year, dayOfYear, dayOfmonth, weekOfYear \n",
    "df_intern['hour'] = df_intern['chargingstart'].dt.hour\n",
    "df_intern['weekday'] = df_intern['chargingstart'].dt.dayofweek\n",
    "df_intern['month'] = df_intern['chargingstart'].dt.month\n",
    "df_intern['year'] = df_intern['chargingstart'].dt.year\n",
    "\n",
    "#################################### Rename attribute ####################################\n",
    "df_intern.rename(columns={'chargingstart': 'date'}, inplace=True)\n",
    "df_extern.rename(columns={'MESS_DATUM': 'temperature'}, inplace=True)\n",
    "df_extern.rename(columns={'TT_TU': 'temperature'}, inplace=True)\n",
    "df_extern.rename(columns={'R1': 'precipitation'}, inplace=True)\n",
    "df_extern.rename(columns={'RS_IND': 'bool_precipitation'}, inplace=True)\n",
    "df_extern.rename(columns={'SD_SO': 'sunshine'}, inplace=True)\n",
    "\n",
    "############ find most counted charging station in data and filter data by that  ############\n",
    "#find\n",
    "item_counts = df_intern[\"chargingstation\"].value_counts(normalize=True)\n",
    "print(item_counts)\n",
    "#filter \n",
    "df_intern = df_intern[(df_intern['chargingstation'] =='G120')]\n",
    "\n",
    "\n",
    "#################################### filter anomaly  ####################################\n",
    "##find epsilon param for DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import pyplot as plt\n",
    "##make date attr numeric to transform to numpy\n",
    "df_intern['date'] = pd.to_numeric(df_intern['date'])\n",
    "X = df_intern[['date', 'chargingtime']].to_numpy()\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=4)\n",
    "nbrs = neigh.fit(X)\n",
    "distances,indices = nbrs.kneighbors(X)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "plt.show()\n",
    "print(\"The optimal value for epsilon will be found at the point of maximum curvature.\")\n",
    "###find maximum curvature\n",
    "arr = np.zeros((len(distances),2))\n",
    "for i in range(len(arr_zeros)):\n",
    "    arr[i][0] = i\n",
    "    arr[i][1] = distances[i]\n",
    "    \n",
    "from kneebow.rotor import Rotor\n",
    "rotor = Rotor()\n",
    "rotor.fit_rotate(arr)\n",
    "epsilon = rotor.get_elbow_index()\n",
    "print(\"maximum curvature = \", distances[epsilon])\n",
    "##run DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##prepare data for model \n",
    "dbscan_data = intern_data[['date', 'chargingtime']]\n",
    "df = dbscan_data\n",
    "##cause dbscan acept only floats make chargingstart numeric\n",
    "dbscan_data['date']= pd.to_numeric(dbscan_data['date'])\n",
    "##normalize Data \n",
    "dbscan_data_scaler = StandardScaler().fit(dbscan_data)\n",
    "dbscan_data = dbscan_data_scaler.transform(dbscan_data)\n",
    "X = dbscan_data\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()\n",
    "\n",
    "clustering = DBSCAN(eps=0.4, min_samples=4).fit(X)\n",
    "print(clustering.labels_)\n",
    "dbscan = DBSCAN(eps = 0.4, min_samples = 4)\n",
    "print(dbscan)\n",
    "\n",
    "pred = dbscan.fit_predict(X)\n",
    "anom_index = where(pred == -1)\n",
    "print(\"anomalys = \", np.count_nonzero(anom_index))\n",
    "values = X[anom_index]\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(values[:,0], values[:,1], color='r')\n",
    "plt.xlabel(\"normalize date\")\n",
    "plt.ylabel(\"normalize chargingtime\")\n",
    "plt.show()\n",
    "\n",
    "##mark all Anomalys\n",
    "ANOMALY = 888\n",
    "lenArr = np.count_nonzero(anom_index)    \n",
    "for i in range(lenArr):\n",
    "    inter_data.iloc[anom_index[0][i],inter_data.columns.get_loc('Anomaly')] = ANOMALY\n",
    "##Filter Anomalys        \n",
    "inter_data = inter_data[(inter_data['Anomaly'] !=ANOMALY)]    \n",
    "\n",
    "#################################### delete not relevant features ####################################\n",
    "df_intern = df_intern.drop(['id', 'ocpptransactionid','created','authenticationtype',\n",
    "                           'validationresult','isbillable','rfid','sigbeginmetervalue',\n",
    "                           'sigendmetervalue','emppartner','cpopartner','platformname',\n",
    "                           'clearingpartner','tariffuid','tariffname','chargepointmodel',\n",
    "                           'chargepointvendor','ocppversion','firmwareversion','iccid',\n",
    "                           'sim_iccid','imsi','sim_imsi','evseid','evseid','evseid',\n",
    "                           'evseid','evseid','evseid','evseid','dsozaehlernummer',\n",
    "                           'meterpublickey','chargingprotocol','voltage','current'\n",
    "                           'chargingprotocol','maxamp', 'connectortype','chargingstop',\n",
    "                           'meterstart','meterstop','connectorid','chargingtype','phase','power',\n",
    "                           'longitude','latitude','country','postalcode','city','region','street',\n",
    "                            'housenumber','chargingtime'\n",
    "                           ], axis = 1)\n",
    "\n",
    "df_extern = df_extern.drop(['Unnamed: 0_x', 'STATIONS_ID_x','QN_9','eor_x',\n",
    "                           'Unnamed: 0_y','STATIONS_ID_y','QN_8','eor_y',\n",
    "                           'Unnamed: 0','STATIONS_ID','QN_7','eor','WRTR','RF_TU' \n",
    "                           ], axis = 1)\n",
    "\n",
    "#################################### scaling ####################################\n",
    "#resample and sum chargingtime\n",
    "df_intern = df_intern.resample('H').agg(dict(chargingtime='sum'))\n",
    "#distribute chargingtime values\n",
    "for i in range(len(df_intern)-1):\n",
    "    chargingtime = df_intern.iloc[i,df_intern.columns.get_loc('chargingtime')]\n",
    "    if(chargingtime>120):\n",
    "        df_intern.iloc[i,df_intern.columns.get_loc('chargingtime')] = 120\n",
    "        df_intern.iloc[i+1,df_intern.columns.get_loc('chargingtime')] = chargingtime-120\n",
    "        \n",
    "#################################### create new atrributes ####################################    \n",
    "#create new attributes hour, weekday, month and year\n",
    "df_intern['hour'] = df_intern['date'].dt.hour\n",
    "df_intern['weekday'] = df_intern['date'].dt.dayofweek\n",
    "df_intern['month'] = df_intern['date'].dt.month\n",
    "df_intern['year'] = df_intern['date'].dt.year\n",
    "#create new attribute weekend \n",
    "INIT = 99\n",
    "df_intern['weekend'] = INIT\n",
    "for i in range(len(df)):\n",
    "    weekday = df_intern.iloc[i,df_intern.columns.get_loc('weekday')]\n",
    "    if(weekday >= 5 ):\n",
    "        df_intern.iloc[i,df_intern.columns.get_loc('weekend')] = 1 # TRUE\n",
    "    else:\n",
    "        df_intern.iloc[i,df_intern.columns.get_loc('weekend')] = 0 # FALSE\n",
    "#create occupation\n",
    "CHARGING_POINT = df_intern['charging_point']\n",
    "df_intern['occupation'] = (df_intern['chargingtime'] / (CHARGING_POINT*60))\n",
    "\n",
    "#################################### merge intern and extern data ####################################   \n",
    "df = pd.merge(df_intern, df_extern, on=\"date\",how=\"left\")\n",
    "\n",
    "#################################### Datareduction with PCA ####################################   \n",
    "#drop not necessary attributes\n",
    "data_pca = df.drop(['date','occupation',\n",
    "                           ], axis = 1)\n",
    "#Principle Component Analysis PCA\n",
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=1) #n_components = number of components\n",
    "pca.fit(data_pca)\n",
    "#pca.components_\n",
    "compresse_data = pca.transform(X)\n",
    "#check shape of compresse_data\n",
    "print(compresse_data.shape)\n",
    "# add new comptressed attribute to df \n",
    "df['compresse_data'] = compresse_data\n",
    "\n",
    "\n",
    "#################################### Correltion between data #################################### \n",
    "print(df.corr(method='spearman'))\n",
    "\n",
    "#################################### extract Trend #################################### \n",
    "from random import randrange\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "series = df['occupation']\n",
    "result = seasonal_decompose(series.tail(1000), model='additive', period=50)\n",
    "result.plot()\n",
    "pyplot.show()\n",
    "\n",
    "#################################### extract Trend #####################\n",
    "from random import randrange\n",
    "from pandas import Series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "series = df['occupation']\n",
    "result = seasonal_decompose(series, model='additive', period=100)\n",
    "result.plot()\n",
    "pyplot.show()\n",
    "\n",
    "#################################### Autocorrelation for attribute occupation #####################\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "pacf_plot = plot_pacf(df.occupation, lags = 1000)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
